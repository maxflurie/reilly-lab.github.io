---
title: "Equivalence Testing"
author: "Jamie Reilly, Ph.D."
date: "`r format(Sys.Date())`"
output:
   html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    css: style.css
---

```{r setup, include=FALSE }
knitr::opts_chunk$set(fig.width=6, fig.height=4, fig.path='Figs/', echo=TRUE, tidy=TRUE, message=F, warning=F, cache=T)
```

```{r}
library(equivalence)
library(tidyverse)
library(reshape2)
```
#Introduction
Many of us are steeped in the tradition of Null Hypothesis Significance Testing (NHST). Under the null hypothesis (H0), there is no difference between conditions. The null hypothesis is that kids from Philly do not differ from kids from NYC by spelling ability. The alternative hypothesis is that Philly > NYC.  This particular example is a directional or one-tailed hypothesis. 
<br/>

Under NHST, the burden of proof is upon us to reject the null.  That's a problem when we are really interested in evaluating  equivalence. Let's say, for example, you are an EPA hydrologist. You think that Seattle and Portland have about the same frequency of rainfall. Even under Scott Pruitt's EPA, you can't evaluate this hypothesis using a frequentist test of differences. Instead, you need an equivalence test. 
<br/>

Some people use Bayes Factors as oblique evidence for equivalence. Others plead ignorance and report the null as evidence for equivalence. I did this until about two months ago when a savvy reviewer slapped my hand. The good news is that **even you** can test for equivalence.      
<br/>

#Equivalence Testing
##TOST Test: 2 Means
Here's a Test of One-Sided Significance (TOST) that evaluates equivalence between two distributions. Let's first simulate some data that we know to be exactly equal - replicating a single random normal distribution.

```{r}
set.seed(1234)
a <- rnorm(1000, 10, 2)  #1000 observations with a mean of 10 and an SD=2
b <- a
c <- data.frame(a,b)
head(c)
```

Variables 'a' and 'b' sure look equivalent. Now we need a statistic.  In standard NHST, the null hypothesis is the no difference hypothesis. If you 'accept' the null, are you justified in concluding that the distributions are equivalent?  
<br/>

No! Absence of difference is not the same as equivalence. It's an easy fallacy to fall into -- much like our legal system where the system works from the assumption of innocence (...ahem, sure).
<br/>

With equivalence testing, the null hypothesis is flipped on its head. That is, you start with the assumption of a difference, and you must then overcome the burden of proof to show equivalence (i.e., p<.05 indicates equivalence not difference). As a sanity check, let's run a TOST test on the two identical distributions we generated above -- assuming they are independent. 
```{r}
tost(c$a, c$b, epsilon = 1, paired = F, var.equal = T, conf.level = 0.95)
```

Yes! That's one fine kettle of fish. We now have a p-value to support our suspicion of equivalence. Let's space the distributions by holding the variance constant (sd=2) and shifting the mean of variable 'b'.  Here's what the distributions look like when mean a=10, mean b=11 by density plot

```{r}
jamie.theme <- theme_bw() + theme(axis.line = element_line(colour = "black"), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), panel.border = element_blank(), panel.background = element_blank(), legend.title= element_blank())
set.seed(1234)
a <- rnorm(1000, 10, 2) 
z <- rnorm(1000, 11, 2)  #now the z-variable has a mean of 11, the two distributions will start to pull apart.
d <- data.frame(a,z)
d.11 <- d %>% melt(measure.vars=1:2)

dense11 <-  ggplot(d.11, aes(x=value, fill=variable)) + geom_density(alpha=.5, colour="white") + jamie.theme
print(dense11)
```

Are distributions 'a' and 'z' equivalent? Let's check with a TOST test. 

```{r}
tost(d$a, d$z, epsilon = 1, paired = F, var.equal = T, conf.level = 0.95)
```

No!  You fail to reject the null. That means you cannot conclude that 'a' and 'b' are equivalent. Note the wording here -- the paradox is that 'not equivalent' does not mean the same thing as different. I know. I know. semantics!  
<br/>

Now just as another sanity check, let's run a standard t-test on a:z. 

```{r}
t.test(d$a, d$z)
```
<br/>

Here are some real data from a study we are working on now where we examined very subtle pupillary responses.  It is important to control for blink rates in this type of experiment. During peer review, a referee raised the logical question of whether were more blinks as participants completed the task of listening for a particular word while sitting in a dark room vs. a bright room.
We hypothesized that blink rates would be equivalent because the task is equally demanding in both lighting conditions but we need evidence! Enter the TOST test. 

<br/>
Here are some analyses of mean blink rate (number of samples with 0mm values / total number of samples) per participant, per lighting condition. 

```{r}
exp1 <- read.csv("Blinks_Exp1.csv", header=T)
exp2 <- read.csv("Blinks_Exp2.csv", header=T)
head(exp1)
head(exp2)
```






